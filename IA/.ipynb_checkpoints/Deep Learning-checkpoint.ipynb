{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "_“Deep learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but nonlinear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. [. . . ] The key aspect of deep learning is that these layers are not designed by human engineers: they are learned from data using a general-purpose learning procedure”_ \n",
    "\n",
    "– Yann LeCun, Yoshua Bengio, and\n",
    "Geoffrey Hinton, Nature 2015\n",
    "\n",
    "The central goal of AI is to provide a set of algorithms and techniques that can be used to solve problems that humans perform intuitively and near automatically, but are otherwise very challenging for computers. A great example of such a class of AI problems is interpreting and understanding the contents of an image – this task is something that a human can do with little-to-no effort, but it has proven to be extremely difficult for machines to accomplish.\n",
    "\n",
    "\n",
    "## ANNs\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a class of machine learning algorithms that learn from data and specialize in pattern recognition, inspired by the structure and function of the brain. The word “neural” is the adjective form of “neuron”, and “network” denotes a graph-like structure; therefore, an “Artificial Neural Network” is a computation system that attempts to mimic (or at least, is inspired by) the neural connections in our nervous system.\n",
    "\n",
    "Our brain is composed by neurons, which we would describe using a binary operation: once exposed to external inputs, the neuron is _fired_ or not, without different 'grades' of _firing_. ANNs are described using models:\n",
    "\n",
    "<img src=\"models.png\">\n",
    "\n",
    "values _x1, x2 and x3_ are inputs to the NN, while constant 1 is called bias, necessary to avoid poor fit results. Each input is connected to the neuron using weights, and the product between both inputs and weights is 'evaluated' by the neuron to trigger or not the output. One typical math notation to the output is:  \n",
    "\n",
    "> out = f(w1*x1 + w2*x2 + ... + wn*xn)  \n",
    "\n",
    "\n",
    "About the activation function, they vary depending of both application and algorithm to implement, being the most common the ones below:\n",
    "\n",
    "<img src=\"ia-activation-functions.png\">\n",
    "\n",
    "\n",
    "## Perceptron Pseudocode\n",
    "\n",
    "1. Initialize the weight vector w with small random values\n",
    "2. Until Perceptron converges:\n",
    "    (a) Loop over each feature vector x j and true class label d i in our training set D\n",
    "    (b) Take x and pass it through the network, calculating the output value: y j = f (w(t) · x j )\n",
    "    (c) Update the weights w: w_i (t + 1) = w_i (t) + α(d_j − y_j )x_(j,i) for all features 0 <= i <= n\n",
    "\n",
    "\n",
    "The value α is the learning rate, if set too higher we would go in the 'right' direction to solve the problem but with the risk to go down into a non-optimal local/global minimum; while if it is too slow may give us a non-practical time to solve the solution.\n",
    "\n",
    "The perceptron training is finished once all trainings samples are classified correctly or after an amount of iterations or _epochs_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a simple perceptron\n",
    "\n",
    "# import the necessary packages\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, N, alpha=0.1):\n",
    "        # initialize the weight matrix and store the learning rate\n",
    "        self.W = np.random.randn(N + 1) / np.sqrt(N)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self, x):\n",
    "        # apply the step function\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def fit(self, X, y, epochs=10):\n",
    "        # insert a column of 1's as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point\n",
    "            for (x, target) in zip(X, y):\n",
    "                # take the dot product between the input features\n",
    "                # and the weight matrix, then pass this value\n",
    "                # through the step function to obtain the prediction\n",
    "                p = self.step(np.dot(x, self.W))\n",
    "\n",
    "                # only perform a weight update if our prediction\n",
    "                # does not match the target\n",
    "                if p != target:\n",
    "                    # determine the error\n",
    "                    error = p - target\n",
    "\n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.alpha * error * x\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # ensure our input is a matrix\n",
    "        X = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # take the dot product between the input features and the\n",
    "        # weight matrix, then pass the value through the step\n",
    "        # function\n",
    "        return self.step(np.dot(X, self.W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: OR and XOR datasets\n",
    "\n",
    "<img src=\"or-xor-datasets.png\">\n",
    "\n",
    "Let's apply the perceptron to solve the OR dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# define our perceptron and train it\n",
    "print(\"training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our perceptron is trained we can evaluate it\n",
    "print(\"testing perceptron...\")\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# define our perceptron and train it\n",
    "print(\"training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our perceptron is trained we can evaluate it\n",
    "print(\"testing perceptron...\")\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# define our perceptron and train it\n",
    "print(\"training perceptron...\")\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our perceptron is trained we can evaluate it\n",
    "print(\"testing perceptron...\")\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Network\n",
    "\n",
    "The most common NN architecture, there are not backward-layers and a connection is implemented only from nodes in layer _i_ to layer _i+1_.\n",
    "\n",
    "<img src=\"ffnn.png\">\n",
    "\n",
    "To describe a feedforward network, we normally use a sequence of integers to quickly and concisely denote the number of nodes in each layer. For example, the network in Figure above is a 3-2-3-2 feedforward network.\n",
    "\n",
    "FN are based on the backpropagation algorithm defined in two phases:\n",
    "\n",
    "**1.** The forward pass where our inputs are passed through the network and output predictions obtained (also known as the propagation phase).\n",
    "\n",
    "\n",
    "To propagate the values through the network and obtain the final classification, we need to take the dot product between the inputs and the weight values, followed by applying an activation function σ().\n",
    "\n",
    "<img src=\"back-propagation.png\" width=\"650px\" height=\"auto\">\n",
    "\n",
    "Assuming that our activation function is the sigmoid:\n",
    "\n",
    "**First layer:**\n",
    "\n",
    "- σ ((0 × 0.351) + (1 × 1.076) + (1 × 1.116)) = 0.899\n",
    "- σ ((0 × −0.097) + (1 × −0.165) + (1 × 0.542)) = 0.593\n",
    "- σ ((0 × 0.457) + (1 × −0.165) + (1 × −0.331)) = 0.378\n",
    "\n",
    "**Out layer:**\n",
    "\n",
    "- σ ((0.899 × 0.383) + (0.593 × −0.327) + (0.378 × −0.329)) = 0.506\n",
    "\n",
    "**2.** The backward pass where we compute the gradient of the loss function at the final layer (i.e., predictions layer) of the network and use this gradient to recursively apply the chain rule to update the weights in our network (also known as the weight update phase).\n",
    "\n",
    "<img src=\"backward-pass-diferential.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "        # that `x` has already been passed through the `sigmoid`\n",
    "        # function\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # insert a column of 1's as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "\n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            # our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "\n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
    "                    epoch + 1, loss))\n",
    "\n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "\n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "\n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # non-linear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "\n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error = A[-1] - y\n",
    "\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas `D`; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    "\n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a `for` loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the non-linear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "\n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value `p`\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a non-linear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "\n",
    "        # return the predicted value\n",
    "        return p\n",
    "\n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try again\n",
    "\n",
    "# construct the XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# define our 2-2-1 neural network and train it\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "# now that our network is trained, loop over the XOR data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST HandWritten Example\n",
    "\n",
    "<img src=\"handwritten.png\">\n",
    "\n",
    "The Mnist handwritten dataset includes 1,797 example digits, each of which are 8 × 8 grayscale images, that once flatened, becomes to a 8x8 = 64-dim vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the MNIST dataset and apply min/max scaling to scale the\n",
    "# pixel intensity values to the range [0, 1] (each image is\n",
    "# represented by an 8 x 8 = 64-dim feature vector)\n",
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],\n",
    "    data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the image to see how this array 'looks'\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "    digits.target, test_size=0.25)\n",
    "\n",
    "trainY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors, more info: https://blog.contactsunny.com/data-science/label-encoder-vs-one-hot-encoder-in-machine-learning\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "nn.fit(trainX, trainY, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Framework: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST (full) dataset...\n"
     ]
    }
   ],
   "source": [
    "# grab the MNIST dataset (if this is your first time running this\n",
    "# script, the download may take a minute -- the 55MB MNIST dataset\n",
    "# will be downloaded)\n",
    "print(\"[INFO] loading MNIST (full) dataset...\")\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\n",
    "# building the input vector from the 28x28 pixels\n",
    "trainX = trainX.reshape(60000, 784)\n",
    "testX = testX.reshape(10000, 784)\n",
    "trainX = trainX.astype('float32')\n",
    "testX = testX.astype('float32')\n",
    "\n",
    "trainX /= 255\n",
    "testX /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"keras-layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 784-256-128-10 architecture using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\"))\n",
    "model.add(Dense(128, activation=\"sigmoid\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 2.2802 - acc: 0.1703 - val_loss: 2.2423 - val_acc: 0.1635\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 2.2082 - acc: 0.3902 - val_loss: 2.1663 - val_acc: 0.4171\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 2.1203 - acc: 0.5350 - val_loss: 2.0607 - val_acc: 0.5272\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 1.9925 - acc: 0.5919 - val_loss: 1.9034 - val_acc: 0.5946\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 1.8105 - acc: 0.6272 - val_loss: 1.6916 - val_acc: 0.6615\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 1.5874 - acc: 0.6566 - val_loss: 1.4602 - val_acc: 0.6786\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.3676 - acc: 0.6935 - val_loss: 1.2546 - val_acc: 0.7124\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.1849 - acc: 0.7260 - val_loss: 1.0941 - val_acc: 0.7466\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 1.0441 - acc: 0.7542 - val_loss: 0.9724 - val_acc: 0.7691\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.9365 - acc: 0.7767 - val_loss: 0.8777 - val_acc: 0.7919\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.8522 - acc: 0.7946 - val_loss: 0.8037 - val_acc: 0.8009\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.7846 - acc: 0.8079 - val_loss: 0.7426 - val_acc: 0.8179\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.7292 - acc: 0.8191 - val_loss: 0.6917 - val_acc: 0.8283\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.6827 - acc: 0.8295 - val_loss: 0.6504 - val_acc: 0.8328\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.6435 - acc: 0.8370 - val_loss: 0.6135 - val_acc: 0.8414\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.6103 - acc: 0.8431 - val_loss: 0.5820 - val_acc: 0.8499\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.5815 - acc: 0.8493 - val_loss: 0.5555 - val_acc: 0.8548\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.5567 - acc: 0.8544 - val_loss: 0.5323 - val_acc: 0.8589\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.5352 - acc: 0.8592 - val_loss: 0.5124 - val_acc: 0.8650\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.5162 - acc: 0.8638 - val_loss: 0.4943 - val_acc: 0.8684\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.4995 - acc: 0.8674 - val_loss: 0.4788 - val_acc: 0.8717\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.4846 - acc: 0.8701 - val_loss: 0.4650 - val_acc: 0.8752\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4714 - acc: 0.8733 - val_loss: 0.4519 - val_acc: 0.8768\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4594 - acc: 0.8760 - val_loss: 0.4407 - val_acc: 0.8820\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.4486 - acc: 0.8787 - val_loss: 0.4302 - val_acc: 0.8837\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4388 - acc: 0.8807 - val_loss: 0.4214 - val_acc: 0.8859\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.4299 - acc: 0.8831 - val_loss: 0.4123 - val_acc: 0.8879\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.4218 - acc: 0.8846 - val_loss: 0.4047 - val_acc: 0.8889\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.4144 - acc: 0.8859 - val_loss: 0.3985 - val_acc: 0.8899\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.4077 - acc: 0.8876 - val_loss: 0.3914 - val_acc: 0.8918\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.4013 - acc: 0.8893 - val_loss: 0.3860 - val_acc: 0.8930\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3955 - acc: 0.8903 - val_loss: 0.3801 - val_acc: 0.8940\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3901 - acc: 0.8916 - val_loss: 0.3751 - val_acc: 0.8953\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3851 - acc: 0.8923 - val_loss: 0.3702 - val_acc: 0.8968\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3803 - acc: 0.8936 - val_loss: 0.3655 - val_acc: 0.8970\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3759 - acc: 0.8945 - val_loss: 0.3617 - val_acc: 0.8987\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3718 - acc: 0.8958 - val_loss: 0.3580 - val_acc: 0.8988\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3679 - acc: 0.8965 - val_loss: 0.3542 - val_acc: 0.8992\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3643 - acc: 0.8974 - val_loss: 0.3502 - val_acc: 0.9003\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3608 - acc: 0.8980 - val_loss: 0.3470 - val_acc: 0.9011\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3575 - acc: 0.8987 - val_loss: 0.3438 - val_acc: 0.9016\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.3543 - acc: 0.8997 - val_loss: 0.3411 - val_acc: 0.9021\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.3513 - acc: 0.9003 - val_loss: 0.3381 - val_acc: 0.9025\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3485 - acc: 0.9007 - val_loss: 0.3354 - val_acc: 0.9030\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3458 - acc: 0.9019 - val_loss: 0.3330 - val_acc: 0.9032\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3431 - acc: 0.9021 - val_loss: 0.3301 - val_acc: 0.9042\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3406 - acc: 0.9024 - val_loss: 0.3280 - val_acc: 0.9046\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.3382 - acc: 0.9037 - val_loss: 0.3254 - val_acc: 0.9053\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3358 - acc: 0.9040 - val_loss: 0.3236 - val_acc: 0.9064\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.3336 - acc: 0.9045 - val_loss: 0.3213 - val_acc: 0.9065\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3314 - acc: 0.9052 - val_loss: 0.3198 - val_acc: 0.9075\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3293 - acc: 0.9058 - val_loss: 0.3177 - val_acc: 0.9077\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3273 - acc: 0.9062 - val_loss: 0.3155 - val_acc: 0.9079\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3252 - acc: 0.9065 - val_loss: 0.3142 - val_acc: 0.9095\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3234 - acc: 0.9074 - val_loss: 0.3118 - val_acc: 0.9100\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.3215 - acc: 0.9077 - val_loss: 0.3104 - val_acc: 0.9108\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3196 - acc: 0.9079 - val_loss: 0.3089 - val_acc: 0.9116\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3178 - acc: 0.9091 - val_loss: 0.3067 - val_acc: 0.9125\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.3162 - acc: 0.9095 - val_loss: 0.3052 - val_acc: 0.9115\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.3145 - acc: 0.9095 - val_loss: 0.3038 - val_acc: 0.9128\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3129 - acc: 0.9102 - val_loss: 0.3027 - val_acc: 0.9133\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3113 - acc: 0.9109 - val_loss: 0.3008 - val_acc: 0.9135\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.3097 - acc: 0.9107 - val_loss: 0.2990 - val_acc: 0.9136\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.3082 - acc: 0.9111 - val_loss: 0.2981 - val_acc: 0.9148\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.3067 - acc: 0.9120 - val_loss: 0.2965 - val_acc: 0.9149\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3052 - acc: 0.9121 - val_loss: 0.2949 - val_acc: 0.9153\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3038 - acc: 0.9128 - val_loss: 0.2939 - val_acc: 0.9158\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3024 - acc: 0.9127 - val_loss: 0.2926 - val_acc: 0.9165\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3010 - acc: 0.9133 - val_loss: 0.2915 - val_acc: 0.9169\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2997 - acc: 0.9140 - val_loss: 0.2902 - val_acc: 0.9173\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2983 - acc: 0.9145 - val_loss: 0.2884 - val_acc: 0.9174\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.2970 - acc: 0.9148 - val_loss: 0.2877 - val_acc: 0.9174\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2958 - acc: 0.9150 - val_loss: 0.2864 - val_acc: 0.9179\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2946 - acc: 0.9155 - val_loss: 0.2850 - val_acc: 0.9185\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2933 - acc: 0.9157 - val_loss: 0.2843 - val_acc: 0.9187\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2921 - acc: 0.9163 - val_loss: 0.2836 - val_acc: 0.9185\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2909 - acc: 0.9162 - val_loss: 0.2821 - val_acc: 0.9194\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.2897 - acc: 0.9164 - val_loss: 0.2814 - val_acc: 0.9190\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2886 - acc: 0.9167 - val_loss: 0.2800 - val_acc: 0.9199\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2874 - acc: 0.9168 - val_loss: 0.2792 - val_acc: 0.9205\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.2863 - acc: 0.9175 - val_loss: 0.2783 - val_acc: 0.9204\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2852 - acc: 0.9176 - val_loss: 0.2772 - val_acc: 0.9203\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2841 - acc: 0.9179 - val_loss: 0.2759 - val_acc: 0.9213\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2830 - acc: 0.9181 - val_loss: 0.2749 - val_acc: 0.9208\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2821 - acc: 0.9181 - val_loss: 0.2742 - val_acc: 0.9213\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2810 - acc: 0.9187 - val_loss: 0.2729 - val_acc: 0.9215\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2799 - acc: 0.9192 - val_loss: 0.2722 - val_acc: 0.9216\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2789 - acc: 0.9194 - val_loss: 0.2710 - val_acc: 0.9218\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2779 - acc: 0.9194 - val_loss: 0.2702 - val_acc: 0.9220\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2769 - acc: 0.9197 - val_loss: 0.2700 - val_acc: 0.9216\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2759 - acc: 0.9202 - val_loss: 0.2686 - val_acc: 0.9227\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2749 - acc: 0.9204 - val_loss: 0.2682 - val_acc: 0.9230\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2740 - acc: 0.9206 - val_loss: 0.2667 - val_acc: 0.9233\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.2729 - acc: 0.9207 - val_loss: 0.2666 - val_acc: 0.9235\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.2721 - acc: 0.9211 - val_loss: 0.2656 - val_acc: 0.9238\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2712 - acc: 0.9213 - val_loss: 0.2642 - val_acc: 0.9237\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2702 - acc: 0.9215 - val_loss: 0.2638 - val_acc: 0.9242\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2693 - acc: 0.9217 - val_loss: 0.2629 - val_acc: 0.9243\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2684 - acc: 0.9219 - val_loss: 0.2619 - val_acc: 0.9238\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.2675 - acc: 0.9222 - val_loss: 0.2609 - val_acc: 0.9250\n"
     ]
    }
   ],
   "source": [
    "# train the model usign SGD\n",
    "print(\"[INFO] training network...\")\n",
    "sgd = SGD(0.01)  # Learning rate 0.01\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd,\n",
    "    metrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "    epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       980\n",
      "           1       0.97      0.98      0.97      1135\n",
      "           2       0.92      0.91      0.91      1032\n",
      "           3       0.91      0.91      0.91      1010\n",
      "           4       0.92      0.93      0.93       982\n",
      "           5       0.90      0.86      0.88       892\n",
      "           6       0.93      0.94      0.94       958\n",
      "           7       0.94      0.93      0.93      1028\n",
      "           8       0.89      0.90      0.89       974\n",
      "           9       0.91      0.91      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.93      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions.argmax(axis=1),\n",
    "    target_names=[str(x) for x in lb.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f73da799198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves and load models\n",
    "model.save(\"mnist-udi.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_1 = load_model(\"mnist-udi.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       980\n",
      "           1       0.97      0.98      0.97      1135\n",
      "           2       0.92      0.91      0.91      1032\n",
      "           3       0.91      0.91      0.91      1010\n",
      "           4       0.92      0.93      0.93       982\n",
      "           5       0.90      0.86      0.88       892\n",
      "           6       0.93      0.94      0.94       958\n",
      "           7       0.94      0.93      0.93      1028\n",
      "           8       0.89      0.90      0.89       974\n",
      "           9       0.91      0.91      0.91      1009\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.93      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model_1.predict(testX, batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions.argmax(axis=1),\n",
    "    target_names=[str(x) for x in lb.classes_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
